{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6811a4",
   "metadata": {},
   "source": [
    "# ðŸ§ª Lab 3: Use an MCP Server as a Plugin in a Semantic Kernel Agent\n",
    "\n",
    "In this lab, you'll learn how to **extend a Semantic Kernel agent** by connecting it to an **MCP server**. MCP (Model Context Protocol) allows agents to invoke external tools, services, or other agents as plugins.\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "You'll specifically:\n",
    "- Connect to **your own Codebeamer MCP server** (which you built in **Lab 2**) as a tool via `MCPStreamableHttpPlugin`\n",
    "- Create a **Semantic Kernel agent** powered by Azure OpenAI\n",
    "- Use the MCP plugin inside the agent to access Codebeamer functionality\n",
    "- Interact with the agent through a chat interface\n",
    "- See the agent automatically call functions on the Codebeamer MCP server to answer questions\n",
    "\n",
    "This lab showcases how Semantic Kernel can leverage **modular, tool-augmented AI workflows** by treating external MCP servers as powerful extensions to the agent's reasoning capabilities.\n",
    "\n",
    "## âœ… Prerequisites\n",
    "\n",
    "Before starting this lab:\n",
    "1. **Complete Lab 2** - You need the Codebeamer MCP server running on `http://localhost:8080`\n",
    "2. **Azure OpenAI credentials** - Ensure your `.env` file is configured with:\n",
    "   - `AZURE_OPENAI_ENDPOINT`\n",
    "   - `AZURE_OPENAI_KEY`\n",
    "   - `MODEL_DEPLOYMENT_NAME`\n",
    "   - `MODEL_DEPLOYMENT_API_VERSION`\n",
    "3. **Start your MCP server** - Run `python3 servers/mcp_server.py` in a terminal\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949ed733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd647cc9",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and Load Environment Variables\n",
    "\n",
    "First, we'll import the necessary Semantic Kernel components and load our Azure OpenAI credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5cb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_OPENAI_ENDPOINT: https://short-hack-ai-foundry.cognitiveservices.azure.com/\n",
      "AZURE_OPENAI_KEY: ***\n",
      "MODEL_DEPLOYMENT_NAME: gpt-4o\n",
      "MODEL_DEPLOYMENT_API_VERSION: 2025-03-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Check environment variables\n",
    "print(\"AZURE_OPENAI_ENDPOINT:\", os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "print(\"AZURE_OPENAI_KEY:\", \"***\" if os.getenv(\"AZURE_OPENAI_KEY\") else \"Not set\")\n",
    "print(\"MODEL_DEPLOYMENT_NAME:\", os.getenv(\"MODEL_DEPLOYMENT_NAME\"))\n",
    "print(\"MODEL_DEPLOYMENT_API_VERSION:\", os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7727f84",
   "metadata": {},
   "source": [
    "## Step 2: Connect to the Codebeamer MCP Server\n",
    "\n",
    "Now we'll initialize and connect to the Codebeamer MCP server using the `MCPStreamableHttpPlugin` interface.\n",
    "\n",
    "**Key points:**\n",
    "- `MCPStreamableHttpPlugin` enables connections to MCP Servers through HTTP/HTTPS\n",
    "- The URL should match your MCP server endpoint (default: `http://localhost:8080/mcp`)\n",
    "- **Important:** Make sure the MCP server built in Lab 2 is running before executing this cell!\n",
    "\n",
    "Run `python3 servers/mcp_server.py` in a separate terminal if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca2a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebeamer_plugin = MCPStreamableHttpPlugin(\n",
    "    name=\"Codebeamer\",\n",
    "    description=\"Codebeamer Plugin\",\n",
    "    url=\"http://localhost:8080/mcp\",\n",
    ")\n",
    "\n",
    "# Start the connection to the MCP plugin\n",
    "await codebeamer_plugin.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9c81b",
   "metadata": {},
   "source": [
    "## Step 3: Create the Azure OpenAI Client and Semantic Kernel Agent\n",
    "\n",
    "In this step, we'll:\n",
    "1. **Create an Azure OpenAI client** using our credentials\n",
    "2. **Initialize a Semantic Kernel agent** (`AzureResponsesAgent`) that:\n",
    "   - Uses Azure OpenAI for language understanding\n",
    "   - Has access to the Codebeamer MCP plugin\n",
    "   - Can automatically invoke Codebeamer tools when needed\n",
    "\n",
    "The agent's instructions define its behavior and capabilities. Feel free to customize these instructions for your use case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d59fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic Kernel agent created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the Azure OpenAI client using credentials from environment variables\n",
    "client = AzureResponsesAgent.create_client(\n",
    "    deployment_name=os.getenv(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "# Create the Semantic Kernel agent with the MCP plugin\n",
    "codebeamer_agent = AzureResponsesAgent(\n",
    "    ai_model_id=os.getenv(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    client=client,\n",
    "    name=\"CodebeamerAgent\",\n",
    "    description=\"A chat bot that helps users interact with Codebeamer.\",\n",
    "    instructions=\"\"\"\n",
    "You are a helpful assistant that can interact with Codebeamer through the provided tools.\n",
    "You can help users:\n",
    "- List and explore projects\n",
    "- View and manage trackers\n",
    "- Get information about tracker items\n",
    "- Read and post comments\n",
    "When a user asks about Codebeamer data, use the available tools to fetch the information.\n",
    "Provide clear, concise, and helpful responses.\n",
    "\"\"\",\n",
    "    plugins=[codebeamer_plugin],\n",
    ")\n",
    "\n",
    "print(\"âœ… Semantic Kernel agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6577477",
   "metadata": {},
   "source": [
    "## Step 4: Chat with Your Agent\n",
    "\n",
    "Now let's run an interactive chat loop! The agent will:\n",
    "- Receive your questions\n",
    "- Automatically decide when to call Codebeamer MCP tools\n",
    "- Maintain conversation context across multiple messages\n",
    "- Provide intelligent responses based on the data retrieved\n",
    "\n",
    "**Try these example questions:**\n",
    "- \"What projects are available?\"\n",
    "- \"Show me the trackers for project ID 12345\"\n",
    "- \"Get items from tracker 5001\"\n",
    "- \"What are the comments on item 5881865?\"\n",
    "- \"Post a comment saying 'Looking good!' on item 5881865\"\n",
    "\n",
    "Type `exit` or `quit` to end the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0aa4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Starting chat with Codebeamer Agent...\n",
      "Type 'exit' or 'quit' to end the conversation.\n",
      "\n",
      "Bot: Here is the project available in Codebeamer:\n",
      "\n",
      "- **Project ID:** 19282  \n",
      "  **Project Name:** Global Hackathon  \n",
      "\n",
      "Let me know if you'd like more details or actions related to this project!\n",
      "\n",
      "Bot: Here are some next steps you can consider:\n",
      "\n",
      "1. **Explore Trackers in the Project:**  \n",
      "   Retrieve the trackers associated with the \"Global Hackathon\" project to see what areas or workflows are defined.  \n",
      "\n",
      "2. **View Tracker Items:**  \n",
      "   Look into individual items within specific trackers (e.g., stories, tasks, requirements).  \n",
      "\n",
      "3. **Check or Add Comments:**  \n",
      "   Check comments for specific tracker items or add your own notes and discussions.  \n",
      "\n",
      "4. **View Project Details:**  \n",
      "   Dive deeper into the project details to learn more about its configuration or objectives.  \n",
      "\n",
      "Which option would you like to explore? Let me know!\n",
      "\n",
      "Goodbye! ðŸ‘‹\n",
      "âœ… Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Create the thread - starts as None, will be set after first response\n",
    "thread = None\n",
    "\n",
    "print(\"ðŸ’¬ Starting chat with Codebeamer Agent...\")\n",
    "print(\"Type 'exit' or 'quit' to end the conversation.\\n\")\n",
    "\n",
    "# Run the agent as a chat\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "\n",
    "    # Get response from the agent\n",
    "    response = await codebeamer_agent.get_response(\n",
    "        messages=user_input,\n",
    "        thread=thread,\n",
    "    )\n",
    "\n",
    "    # Print the response\n",
    "    print(f\"Bot: {response.content}\\n\")\n",
    "    \n",
    "    # Update the thread with the response to maintain conversation context\n",
    "    thread = response.thread\n",
    "\n",
    "# Clean up the connection\n",
    "await codebeamer_plugin.close()\n",
    "print(\"âœ… Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71784718",
   "metadata": {},
   "source": [
    "## ðŸŽ“ What You Learned\n",
    "\n",
    "Congratulations! In this lab, you've successfully:\n",
    "\n",
    "âœ… Connected a Semantic Kernel agent to an external MCP server  \n",
    "âœ… Created a tool-augmented AI agent with Azure OpenAI  \n",
    "âœ… Enabled automatic tool selection and invocation  \n",
    "âœ… Built an interactive chat interface with conversation context  \n",
    "âœ… Integrated custom APIs (Codebeamer) into an AI workflow  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps & Exploration\n",
    "\n",
    "### Extend Your Agent\n",
    "\n",
    "You can now:\n",
    "- **Add more MCP plugins** - Connect multiple MCP servers to create a multi-tool agent\n",
    "- **Customize instructions** - Tailor the agent's behavior for specific use cases\n",
    "- **Build workflows** - Chain multiple tool calls together for complex tasks\n",
    "- **Add memory** - Implement persistent conversation history\n",
    "\n",
    "### Explore Other MCP Servers\n",
    "\n",
    "The real power comes from the flexibility of MCP plugins in Semantic Kernel. You can connect to **any tool that implements the Model Context Protocol (MCP)**.\n",
    "\n",
    "ðŸ§° **Explore These MCP Servers:**\n",
    "\n",
    "- ðŸ”— [Official MCP Server Integrations (GitHub)](https://github.com/modelcontextprotocol/servers?tab=readme-ov-file#%EF%B8%8F-official-integrations)  \n",
    "  A growing list of plugins including GitHub, Slack, Google Drive, PostgreSQL, and more.\n",
    "\n",
    "- ðŸ“š [10 Must-Know MCP Servers for Developers (DevShorts)](https://www.devshorts.in/p/ten-must-know-mcp-servers-for-every?utm_source=chatgpt.com)  \n",
    "  A curated blog post with descriptions, commands, and usage tips.\n",
    "\n",
    "- ðŸ”§ [FastMCP Documentation](https://gofastmcp.com/)  \n",
    "  Learn more about building your own MCP servers.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Key Concepts Recap\n",
    "\n",
    "### MCPStreamableHttpPlugin\n",
    "- Enables Semantic Kernel to connect to MCP servers via HTTP\n",
    "- Handles tool discovery and invocation automatically\n",
    "- Supports async operations for better performance\n",
    "\n",
    "### AzureResponsesAgent\n",
    "- A Semantic Kernel agent powered by Azure OpenAI\n",
    "- Can accept multiple plugins for tool augmentation\n",
    "- Maintains conversation context with threads\n",
    "- Automatically decides when to invoke tools\n",
    "\n",
    "### Tool-Augmented AI\n",
    "- LLMs can be extended with external tools and APIs\n",
    "- MCP provides a standardized protocol for tool integration\n",
    "- Agents can reason about when and how to use tools\n",
    "- This enables AI to interact with real-world systems and data\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Challenge Ideas\n",
    "\n",
    "Try these extensions to deepen your understanding:\n",
    "\n",
    "1. **Multi-Plugin Agent**: Add the Wikipedia MCP server from Lab 1 alongside Codebeamer\n",
    "2. **Custom Instructions**: Modify the agent to specialize in specific Codebeamer workflows\n",
    "3. **Error Handling**: Add try-catch blocks and graceful error messages\n",
    "4. **Logging**: Track which tools are called and why\n",
    "5. **Build Your Own**: Create a completely new MCP server and integrate it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
