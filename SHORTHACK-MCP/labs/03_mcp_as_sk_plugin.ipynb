{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6811a4",
   "metadata": {},
   "source": [
    "# ðŸ§ª Lab 3: Use an MCP Server as a Plugin in a Semantic Kernel Agent\n",
    "\n",
    "In this lab, you'll learn how to **extend a Semantic Kernel agent** by connecting it to an **MCP server**. MCP (Model Context Protocol) allows agents to invoke external tools, services, or other agents as plugins.\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "You'll specifically:\n",
    "- Connect to **your own Codebeamer MCP server** (which you built in **Lab 2**) as a tool via `MCPStreamableHttpPlugin`\n",
    "- Create a **Semantic Kernel agent** powered by Azure OpenAI\n",
    "- Use the MCP plugin inside the agent to access Codebeamer functionality\n",
    "- Interact with the agent through a chat interface\n",
    "- See the agent automatically call functions on the Codebeamer MCP server to answer questions\n",
    "\n",
    "This lab showcases how Semantic Kernel can leverage **modular, tool-augmented AI workflows** by treating external MCP servers as powerful extensions to the agent's reasoning capabilities.\n",
    "\n",
    "## âœ… Prerequisites\n",
    "\n",
    "Before starting this lab:\n",
    "1. **Complete Lab 2** - You need the Codebeamer MCP server running on `http://localhost:8080`\n",
    "2. **Azure OpenAI credentials** - Ensure your `.env` file is configured with:\n",
    "   - `AZURE_OPENAI_ENDPOINT`\n",
    "   - `AZURE_OPENAI_KEY`\n",
    "   - `MODEL_DEPLOYMENT_NAME`\n",
    "   - `MODEL_DEPLOYMENT_API_VERSION`\n",
    "3. **Start your MCP server** - Run `python3 servers/mcp_server.py` in a terminal\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949ed733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd647cc9",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and Load Environment Variables\n",
    "\n",
    "First, we'll import the necessary Semantic Kernel components and load our Azure OpenAI credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5cb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_OPENAI_ENDPOINT: https://short-hack-ai-foundry.cognitiveservices.azure.com/\n",
      "AZURE_OPENAI_KEY: ***\n",
      "MODEL_DEPLOYMENT_NAME: gpt-4o\n",
      "MODEL_DEPLOYMENT_API_VERSION: 2024-12-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Check environment variables\n",
    "print(\"AZURE_OPENAI_ENDPOINT:\", os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "print(\"AZURE_OPENAI_KEY:\", \"***\" if os.getenv(\"AZURE_OPENAI_KEY\") else \"Not set\")\n",
    "print(\"MODEL_DEPLOYMENT_NAME:\", os.getenv(\"MODEL_DEPLOYMENT_NAME\"))\n",
    "print(\"MODEL_DEPLOYMENT_API_VERSION:\", os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7727f84",
   "metadata": {},
   "source": [
    "## Step 2: Connect to the Codebeamer MCP Server\n",
    "\n",
    "Now we'll initialize and connect to the Codebeamer MCP server using the `MCPStreamableHttpPlugin` interface.\n",
    "\n",
    "**Key points:**\n",
    "- `MCPStreamableHttpPlugin` enables connections to MCP Servers through HTTP/HTTPS\n",
    "- The URL should match your MCP server endpoint (default: `http://localhost:8080/mcp`)\n",
    "- **Important:** Make sure the MCP server built in Lab 2 is running before executing this cell!\n",
    "\n",
    "Run `python3 servers/mcp_server.py` in a separate terminal if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca2a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebeamer_plugin = MCPStreamableHttpPlugin(\n",
    "    name=\"Codebeamer\",\n",
    "    description=\"Codebeamer Plugin\",\n",
    "    url=\"http://localhost:8080/mcp\",\n",
    ")\n",
    "\n",
    "# Start the connection to the MCP plugin\n",
    "await codebeamer_plugin.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9c81b",
   "metadata": {},
   "source": [
    "## Step 3: Create the Azure OpenAI Client and Semantic Kernel Agent\n",
    "\n",
    "In this step, we'll:\n",
    "1. **Create an Azure OpenAI client** using our credentials\n",
    "2. **Initialize a Semantic Kernel agent** (`AzureResponsesAgent`) that:\n",
    "   - Uses Azure OpenAI for language understanding\n",
    "   - Has access to the Codebeamer MCP plugin\n",
    "   - Can automatically invoke Codebeamer tools when needed\n",
    "\n",
    "The agent's instructions define its behavior and capabilities. Feel free to customize these instructions for your use case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d59fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic Kernel agent created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the Azure OpenAI client using credentials from environment variables\n",
    "client = AzureResponsesAgent.create_client(\n",
    "    deployment_name=os.getenv(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "# Create the Semantic Kernel agent with the MCP plugin\n",
    "codebeamer_agent = AzureResponsesAgent(\n",
    "    ai_model_id=os.getenv(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    client=client,\n",
    "    name=\"CodebeamerAgent\",\n",
    "    description=\"A chat bot that helps users interact with Codebeamer.\",\n",
    "    instructions=\"\"\"\n",
    "You are a helpful assistant that can interact with Codebeamer through the provided tools.\n",
    "You can help users:\n",
    "- List and explore projects\n",
    "- View and manage trackers\n",
    "- Get information about tracker items\n",
    "- Read and post comments\n",
    "When a user asks about Codebeamer data, use the available tools to fetch the information.\n",
    "Provide clear, concise, and helpful responses.\n",
    "\"\"\",\n",
    "    plugins=[codebeamer_plugin],\n",
    ")\n",
    "\n",
    "print(\"âœ… Semantic Kernel agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6577477",
   "metadata": {},
   "source": [
    "## Step 4: Chat with Your Agent\n",
    "\n",
    "Now let's run an interactive chat loop! The agent will:\n",
    "- Receive your questions\n",
    "- Automatically decide when to call Codebeamer MCP tools\n",
    "- Maintain conversation context across multiple messages\n",
    "- Provide intelligent responses based on the data retrieved\n",
    "\n",
    "**Try these example questions:**\n",
    "- \"What projects are available?\"\n",
    "- \"Show me the trackers for project ID 12345\"\n",
    "- \"Get items from tracker 5001\"\n",
    "- \"What are the comments on item 5881865?\"\n",
    "- \"Post a comment saying 'Looking good!' on item 5881865\"\n",
    "\n",
    "Type `exit` or `quit` to end the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0aa4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Starting chat with Codebeamer Agent...\n",
      "Type 'exit' or 'quit' to end the conversation.\n",
      "\n"
     ]
    },
    {
     "ename": "AgentExecutionException",
     "evalue": "(\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError(\"Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Azure OpenAI Responses API is enabled only for api-version 2025-03-01-preview and later'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/semantic_kernel/agents/open_ai/responses_agent_thread_actions.py:609\u001b[39m, in \u001b[36mResponsesAgentThreadActions._get_response\u001b[39m\u001b[34m(cls, agent, chat_history, merged_instructions, previous_response_id, store_output_enabled, tools, response_options, stream)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response: Response = \u001b[38;5;28;01mawait\u001b[39;00m agent.client.responses.create(\n\u001b[32m    610\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28mcls\u001b[39m._prepare_chat_history_for_request(\n\u001b[32m    611\u001b[39m             chat_history, store_output_enabled \u001b[38;5;28;01mif\u001b[39;00m store_output_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m agent.store_enabled\n\u001b[32m    612\u001b[39m         ),\n\u001b[32m    613\u001b[39m         instructions=merged_instructions \u001b[38;5;129;01mor\u001b[39;00m agent.instructions,\n\u001b[32m    614\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    615\u001b[39m         store=store_output_enabled,\n\u001b[32m    616\u001b[39m         tools=tools,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    617\u001b[39m         stream=stream,\n\u001b[32m    618\u001b[39m         **response_options,\n\u001b[32m    619\u001b[39m     )\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequestError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/resources/responses/responses.py:2259\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2223\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2224\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2257\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2258\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2260\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2261\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2262\u001b[39m             {\n\u001b[32m   2263\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2264\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2265\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2266\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2267\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2268\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2269\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2270\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2271\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2272\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2273\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2274\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2275\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2276\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2277\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2278\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2279\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2280\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2281\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2282\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2283\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2284\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2285\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2286\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2287\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2288\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2289\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2290\u001b[39m             },\n\u001b[32m   2291\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2292\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2294\u001b[39m         ),\n\u001b[32m   2295\u001b[39m         options=make_request_options(\n\u001b[32m   2296\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2297\u001b[39m         ),\n\u001b[32m   2298\u001b[39m         cast_to=Response,\n\u001b[32m   2299\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2300\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Azure OpenAI Responses API is enabled only for api-version 2025-03-01-preview and later'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAgentExecutionException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Get response from the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m codebeamer_agent.get_response(\n\u001b[32m     17\u001b[39m     messages=user_input,\n\u001b[32m     18\u001b[39m     thread=thread,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/semantic_kernel/utils/telemetry/agent_diagnostics/decorators.py:72\u001b[39m, in \u001b[36mtrace_agent_get_response.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(get_response_func)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(\n\u001b[32m     67\u001b[39m     *args: P.args,\n\u001b[32m     68\u001b[39m     **kwargs: P.kwargs,\n\u001b[32m     69\u001b[39m ) -> AgentResponseItem[ChatMessageContent]:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m     71\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the responses\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_response_func(*args, **kwargs)\n\u001b[32m     74\u001b[39m     agent = cast(Agent, args[\u001b[32m0\u001b[39m])\n\u001b[32m     75\u001b[39m     messages = args[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/semantic_kernel/agents/open_ai/openai_responses_agent.py:920\u001b[39m, in \u001b[36mOpenAIResponsesAgent.get_response\u001b[39m\u001b[34m(self, messages, thread, arguments, kernel, include, instruction_role, instructions_override, function_choice_behavior, max_output_tokens, metadata, model, parallel_tool_calls, polling_options, reasoning, text, tools, temperature, top_p, truncation, **kwargs)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m function_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m    919\u001b[39m response_messages: \u001b[38;5;28mlist\u001b[39m[ChatMessageContent] = []\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m is_visible, response \u001b[38;5;129;01min\u001b[39;00m ResponsesAgentThreadActions.invoke(\n\u001b[32m    921\u001b[39m     agent=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    922\u001b[39m     chat_history=chat_history,\n\u001b[32m    923\u001b[39m     thread=thread,\n\u001b[32m    924\u001b[39m     store_enabled=\u001b[38;5;28mself\u001b[39m.store_enabled,\n\u001b[32m    925\u001b[39m     kernel=kernel,\n\u001b[32m    926\u001b[39m     arguments=arguments,\n\u001b[32m    927\u001b[39m     function_choice_behavior=function_choice_behavior,\n\u001b[32m    928\u001b[39m     **response_level_params,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    929\u001b[39m ):\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_visible \u001b[38;5;129;01mand\u001b[39;00m response.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    931\u001b[39m         response.metadata[\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m] = thread.id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/semantic_kernel/agents/open_ai/responses_agent_thread_actions.py:184\u001b[39m, in \u001b[36mResponsesAgentThreadActions.invoke\u001b[39m\u001b[34m(cls, agent, chat_history, thread, store_enabled, function_choice_behavior, arguments, include, instructions_override, kernel, max_output_tokens, metadata, model, parallel_tool_calls, polling_options, reasoning, text, tools, temperature, top_p, truncation, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m     previous_response_id = thread.response_id\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m request_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(function_choice_behavior.maximum_auto_invoke_attempts):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_response(\n\u001b[32m    185\u001b[39m         agent=agent,\n\u001b[32m    186\u001b[39m         chat_history=override_history,\n\u001b[32m    187\u001b[39m         merged_instructions=merged_instructions,\n\u001b[32m    188\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    189\u001b[39m         store_output_enabled=store_enabled,\n\u001b[32m    190\u001b[39m         tools=tools,\n\u001b[32m    191\u001b[39m         response_options=response_options,\n\u001b[32m    192\u001b[39m     )\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, Response):\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AgentInvokeException(\u001b[33m\"\u001b[39m\u001b[33mResponse is not of type Response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/semantic_kernel/agents/open_ai/responses_agent_thread_actions.py:626\u001b[39m, in \u001b[36mResponsesAgentThreadActions._get_response\u001b[39m\u001b[34m(cls, agent, chat_history, merged_instructions, previous_response_id, store_output_enabled, tools, response_options, stream)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ex.code == \u001b[33m\"\u001b[39m\u001b[33mcontent_filter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    622\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[32m    623\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m encountered a content error\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    624\u001b[39m             ex,\n\u001b[32m    625\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentExecutionException(\n\u001b[32m    627\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed to complete the request\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    628\u001b[39m         ex,\n\u001b[32m    629\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentExecutionException(\n\u001b[32m    632\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the request\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    633\u001b[39m         ex,\n\u001b[32m    634\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mAgentExecutionException\u001b[39m: (\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError(\"Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Azure OpenAI Responses API is enabled only for api-version 2025-03-01-preview and later'}}\"))"
     ]
    }
   ],
   "source": [
    "# Create the thread - starts as None, will be set after first response\n",
    "thread = None\n",
    "\n",
    "print(\"ðŸ’¬ Starting chat with Codebeamer Agent...\")\n",
    "print(\"Type 'exit' or 'quit' to end the conversation.\\n\")\n",
    "\n",
    "# Run the agent as a chat\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "\n",
    "    # Get response from the agent\n",
    "    response = await codebeamer_agent.get_response(\n",
    "        messages=user_input,\n",
    "        thread=thread,\n",
    "    )\n",
    "\n",
    "    # Print the response\n",
    "    print(f\"Bot: {response.content}\\n\")\n",
    "    \n",
    "    # Update the thread with the response to maintain conversation context\n",
    "    thread = response.thread\n",
    "\n",
    "# Clean up the connection\n",
    "await codebeamer_plugin.close()\n",
    "print(\"âœ… Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71784718",
   "metadata": {},
   "source": [
    "## ðŸŽ“ What You Learned\n",
    "\n",
    "Congratulations! In this lab, you've successfully:\n",
    "\n",
    "âœ… Connected a Semantic Kernel agent to an external MCP server  \n",
    "âœ… Created a tool-augmented AI agent with Azure OpenAI  \n",
    "âœ… Enabled automatic tool selection and invocation  \n",
    "âœ… Built an interactive chat interface with conversation context  \n",
    "âœ… Integrated custom APIs (Codebeamer) into an AI workflow  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps & Exploration\n",
    "\n",
    "### Extend Your Agent\n",
    "\n",
    "You can now:\n",
    "- **Add more MCP plugins** - Connect multiple MCP servers to create a multi-tool agent\n",
    "- **Customize instructions** - Tailor the agent's behavior for specific use cases\n",
    "- **Build workflows** - Chain multiple tool calls together for complex tasks\n",
    "- **Add memory** - Implement persistent conversation history\n",
    "\n",
    "### Explore Other MCP Servers\n",
    "\n",
    "The real power comes from the flexibility of MCP plugins in Semantic Kernel. You can connect to **any tool that implements the Model Context Protocol (MCP)**.\n",
    "\n",
    "ðŸ§° **Explore These MCP Servers:**\n",
    "\n",
    "- ðŸ”— [Official MCP Server Integrations (GitHub)](https://github.com/modelcontextprotocol/servers?tab=readme-ov-file#%EF%B8%8F-official-integrations)  \n",
    "  A growing list of plugins including GitHub, Slack, Google Drive, PostgreSQL, and more.\n",
    "\n",
    "- ðŸ“š [10 Must-Know MCP Servers for Developers (DevShorts)](https://www.devshorts.in/p/ten-must-know-mcp-servers-for-every?utm_source=chatgpt.com)  \n",
    "  A curated blog post with descriptions, commands, and usage tips.\n",
    "\n",
    "- ðŸ”§ [FastMCP Documentation](https://gofastmcp.com/)  \n",
    "  Learn more about building your own MCP servers.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Key Concepts Recap\n",
    "\n",
    "### MCPStreamableHttpPlugin\n",
    "- Enables Semantic Kernel to connect to MCP servers via HTTP\n",
    "- Handles tool discovery and invocation automatically\n",
    "- Supports async operations for better performance\n",
    "\n",
    "### AzureResponsesAgent\n",
    "- A Semantic Kernel agent powered by Azure OpenAI\n",
    "- Can accept multiple plugins for tool augmentation\n",
    "- Maintains conversation context with threads\n",
    "- Automatically decides when to invoke tools\n",
    "\n",
    "### Tool-Augmented AI\n",
    "- LLMs can be extended with external tools and APIs\n",
    "- MCP provides a standardized protocol for tool integration\n",
    "- Agents can reason about when and how to use tools\n",
    "- This enables AI to interact with real-world systems and data\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Challenge Ideas\n",
    "\n",
    "Try these extensions to deepen your understanding:\n",
    "\n",
    "1. **Multi-Plugin Agent**: Add the Wikipedia MCP server from Lab 1 alongside Codebeamer\n",
    "2. **Custom Instructions**: Modify the agent to specialize in specific Codebeamer workflows\n",
    "3. **Error Handling**: Add try-catch blocks and graceful error messages\n",
    "4. **Logging**: Track which tools are called and why\n",
    "5. **Build Your Own**: Create a completely new MCP server and integrate it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
