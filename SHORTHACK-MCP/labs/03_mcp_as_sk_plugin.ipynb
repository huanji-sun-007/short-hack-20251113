{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6811a4",
   "metadata": {},
   "source": [
    "# ðŸ§ª Lab 2: Use an MCP Server as a Plugin in a Semantic Kernel Agent\n",
    "\n",
    "In this lab, you'll learn how to **extend a Semantic Kernel agent** by connecting it to an **external MCP server**. MCP (Model Context Protocol) allows agents to invoke external tools, services, or other agents as plugins.\n",
    "\n",
    "You'll specifically:\n",
    "- Connect to the **GitHub MCP server** as a tool via `MCPStreamableHttpPlugin`\n",
    "- Use this plugin inside a **Semantic Kernel agent**\n",
    "- Interact with the agent through a chat interface\n",
    "- See the agent automatically call functions on the GitHub MCP server to answer questions\n",
    "\n",
    "This lab showcases how Semantic Kernel can leverage **modular, tool-augmented AI workflows** by treating external MCP servers as powerful extensions to the agent's reasoning capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "949ed733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import AsyncAzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5cb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_OPENAI_ENDPOINT: https://sunhu-mhjykg2a-swedencentral.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview\n",
      "AZURE_OPENAI_KEY: ***\n",
      "MODEL_DEPLOYMENT_NAME: gpt-5-mini\n",
      "MODEL_DEPLOYMENT_API_VERSION: 2025-04-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Check environment variables\n",
    "print(\"AZURE_OPENAI_ENDPOINT:\", os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "print(\"AZURE_OPENAI_KEY:\", \"***\" if os.getenv(\"AZURE_OPENAI_KEY\") else \"Not set\")\n",
    "print(\"MODEL_DEPLOYMENT_NAME:\", os.getenv(\"MODEL_DEPLOYMENT_NAME\"))\n",
    "print(\"MODEL_DEPLOYMENT_API_VERSION:\", os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7727f84",
   "metadata": {},
   "source": [
    "The code block below initializes and connects to a GitHub MCP (Model Context Protocol) plugin using the `MCPSsePlugin` interface.\n",
    "\n",
    "- `MCPSsePlugin` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca2a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebeamer_plugin = MCPStreamableHttpPlugin(\n",
    "    name=\"Codebeamer\",\n",
    "    description=\"Codebeamer Plugin\",\n",
    "    url=\"http://localhost:8080/mcp\",\n",
    ")\n",
    "\n",
    "# Start the connection to the MCP plugin\n",
    "await codebeamer_plugin.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9c81b",
   "metadata": {},
   "source": [
    "Now, let's set up an AI agent that can interact with an external MCP server as a plugin.\n",
    "\n",
    "Weâ€™ll first configure the Azure OpenAI chat completion service, then define an agent by giving it a name, description, and a clear set of instructions. \n",
    "\n",
    "Finally, we attach the MCP-based GitHub plugin so the agent can call functions exposed by the plugin to fulfill user requests dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d59fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chat completion service\n",
    "service_id = \"azure_openai_chat\"\n",
    "async_openai_client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"),\n",
    ")\n",
    "\n",
    "chat_service = AzureChatCompletion(\n",
    "    service_id=service_id,\n",
    "    async_client=async_openai_client,\n",
    "    deployment_name=os.getenv(\"MODEL_DEPLOYMENT_NAME\"),\n",
    ")\n",
    "settings = AzureChatPromptExecutionSettings(service_id=service_id)\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create the agent with the chat service and plugin\n",
    "codebeamer_agent = ChatCompletionAgent(\n",
    "    service=chat_service,\n",
    "    name=\"CodebeamerAgent\",\n",
    "    description=\"A chat bot that helps users interact with Codebeamer.\",\n",
    "    instructions=\"\"\"\n",
    "You are a chat bot. And you help users interact with Codebeamer.\n",
    "You are especially good at answering questions about the Microsoft semantic-kernel project.\n",
    "You can call functions to get the information you need.\n",
    "\"\"\",\n",
    "    plugins=[codebeamer_plugin],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6577477",
   "metadata": {},
   "source": [
    "Now let's try it out! ðŸŽ‰\n",
    "\n",
    "We're spinning up a simple chat loop where you can talk directly with the agent â€” powered by Azure OpenAI and enhanced with GitHub superpowers via the MCP plugin.\n",
    "\n",
    "Letâ€™s see this agent in action! ðŸ¤–ðŸ’¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0aa4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You: hello\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"Unsupported parameter: \\'messages\\'. In the Responses API, this parameter has moved to \\'input\\'. Try again with the new parameter. See the API documentation for more information: https://platform.openai.com/docs/api-reference/responses/create.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': \\'unsupported_parameter\\'}}'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:88\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     87\u001b[0m         settings_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 88\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings_dict)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2584\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2587\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2588\u001b[0m         {\n\u001b[1;32m   2589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2590\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2591\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2594\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2596\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2597\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2599\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2600\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2601\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2602\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2603\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2604\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2605\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2606\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2607\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2608\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2609\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2612\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2615\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2619\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2620\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2621\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2622\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2623\u001b[0m         },\n\u001b[1;32m   2624\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2626\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2627\u001b[0m     ),\n\u001b[1;32m   2628\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2629\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2630\u001b[0m     ),\n\u001b[1;32m   2631\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2632\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2633\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2634\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:1794\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1791\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1792\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1793\u001b[0m )\n\u001b[0;32m-> 1794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:1594\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'messages'. In the Responses API, this parameter has moved to 'input'. Try again with the new parameter. See the API documentation for more information: https://platform.openai.com/docs/api-reference/responses/create.\", 'type': 'invalid_request_error', 'param': None, 'code': 'unsupported_parameter'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m codebeamer_agent\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39muser_input,\n\u001b[1;32m     14\u001b[0m     thread\u001b[38;5;241m=\u001b[39mthread,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m assistant_reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(response)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant_reply\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/utils/telemetry/agent_diagnostics/decorators.py:72\u001b[0m, in \u001b[0;36mtrace_agent_get_response.<locals>.wrapper_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(get_response_func)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_decorator\u001b[39m(\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentResponseItem[ChatMessageContent]:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the responses\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_response_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m     agent \u001b[38;5;241m=\u001b[39m cast(Agent, args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     75\u001b[0m     messages \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/agents/chat_completion/chat_completion_agent.py:311\u001b[0m, in \u001b[0;36mChatCompletionAgent.get_response\u001b[0;34m(self, messages, thread, arguments, kernel, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39madd_message(message)\n\u001b[1;32m    310\u001b[0m responses: \u001b[38;5;28mlist\u001b[39m[ChatMessageContent] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_invoke(\n\u001b[1;32m    312\u001b[0m     thread,\n\u001b[1;32m    313\u001b[0m     chat_history,\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m     arguments,\n\u001b[1;32m    316\u001b[0m     kernel,\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    318\u001b[0m ):\n\u001b[1;32m    319\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m responses:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/agents/chat_completion/chat_completion_agent.py:537\u001b[0m, in \u001b[0;36mChatCompletionAgent._inner_invoke\u001b[0;34m(self, thread, history, on_intermediate_message, arguments, kernel, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m message_count_before_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(agent_chat_history)\n\u001b[1;32m    535\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chat_completion_service)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 537\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_completion_service\u001b[38;5;241m.\u001b[39mget_chat_message_contents(\n\u001b[1;32m    538\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39magent_chat_history,\n\u001b[1;32m    539\u001b[0m     settings\u001b[38;5;241m=\u001b[39msettings,\n\u001b[1;32m    540\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[1;32m    541\u001b[0m     arguments\u001b[38;5;241m=\u001b[39marguments,\n\u001b[1;32m    542\u001b[0m )\n\u001b[1;32m    544\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Invoked \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chat_completion_service)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith message count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_count_before_completion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Drain newly added tool messages since last index to maintain\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# correct order and avoid duplicates\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py:139\u001b[0m, in \u001b[0;36mChatCompletionClientBase.get_chat_message_contents\u001b[0;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m use_span(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_auto_function_invocation_activity(kernel, settings), end_on_exit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m _:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m request_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior\u001b[38;5;241m.\u001b[39mmaximum_auto_invoke_attempts):\n\u001b[0;32m--> 139\u001b[0m         completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_get_chat_message_contents(chat_history, settings)\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# Get the function call contents from the chat message. There is only one chat message,\u001b[39;00m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;66;03m# which should be checked in the `_verify_function_choice_settings` method.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         function_calls \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m completions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitems \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, FunctionCallContent)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/utils/telemetry/model_diagnostics/decorators.py:112\u001b[0m, in \u001b[0;36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(completion_func)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[ChatMessageContent]:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     completion_service: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatCompletionClientBase\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    115\u001b[0m     chat_history: ChatHistory \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_chat_completion_base.py:88\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[0;34m(self, chat_history, settings)\u001b[0m\n\u001b[1;32m     85\u001b[0m settings\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat_history_for_request(chat_history)\n\u001b[1;32m     86\u001b[0m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[0;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(settings)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[1;32m     90\u001b[0m response_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_from_chat_response(response)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:60\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mTEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mCHAT:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_completion_request(settings)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mEMBEDDING:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:100\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_filter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service encountered a content error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m             ex,\n\u001b[1;32m     99\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m         ex,\n\u001b[1;32m    103\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    107\u001b[0m         ex,\n\u001b[1;32m    108\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[0;31mServiceResponseException\u001b[0m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"Unsupported parameter: \\'messages\\'. In the Responses API, this parameter has moved to \\'input\\'. Try again with the new parameter. See the API documentation for more information: https://platform.openai.com/docs/api-reference/responses/create.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': \\'unsupported_parameter\\'}}'))"
     ]
    }
   ],
   "source": [
    "# Create the thread\n",
    "thread: ChatHistoryAgentThread = ChatHistoryAgentThread()\n",
    "\n",
    "# Run the agent as a chat\n",
    "while True:\n",
    "    user_input = input(\"Enter your message:\")\n",
    "    print(f\"\\nYou: {user_input}\")\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    response = await codebeamer_agent.get_response(\n",
    "        input=user_input,\n",
    "        thread=thread,\n",
    "    )\n",
    "\n",
    "    assistant_reply = str(response)\n",
    "    print(f\"Bot: {assistant_reply}\")\n",
    "\n",
    "await codebeamer_plugin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71784718",
   "metadata": {},
   "source": [
    "### ðŸ§ª Try it out!\n",
    "\n",
    "In this lab, you used the GitHub MCP Server â€” but the real power comes from the flexibility of the `MCPStdioPlugin` in Semantic Kernel. This component allows you to connect to **any tool that implements the Model Context Protocol (MCP)**.\n",
    "\n",
    "You can easily swap in other MCP-compatible servers, or even chain multiple plugins together to create powerful tool-augmented agents.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ§° **Explore These MCP Plugin Servers:**\n",
    "\n",
    "- ðŸ”— [Official MCP Server Integrations (GitHub)](https://github.com/modelcontextprotocol/servers?tab=readme-ov-file#%EF%B8%8F-official-integrations)  \n",
    "  A growing list of plugins including GitHub, Weather, Jira, Open Interpreter, and more.\n",
    "\n",
    "- ðŸ“š [10 Must-Know MCP Servers for Developers (DevShorts)](https://www.devshorts.in/p/ten-must-know-mcp-servers-for-every?utm_source=chatgpt.com)  \n",
    "  A curated blog post with descriptions, commands, and usage tips.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
